{"columns": ["inputs", "query", "answer"], "data": [["Answer the question based on the context below.\n\nContext:\n.ipynb\n.pdf\nConversationBufferMemory\n Contents \nUsing in a chain\nConversationBufferMemory#\nThis notebook shows how to use ConversationBufferMemory. This memory allows for storing of messages and then extracts the messages in a variable.\nWe can first extract it as a string.\nfrom langchain.memory import ConversationBufferMemory\nmemory = ConversationBufferMemory()\nmemory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\nmemory.load_memory_variables({})\n{'history': 'Human: hi\\nAI: whats up'}\nWe can also get the history as a list of messages (this is useful if you are using this with a chat model).\nmemory = ConversationBufferMemory(return_messages=True)\nmemory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\nmemory.load_memory_variables({})\n{'history': [HumanMessage(content='hi', additional_kwargs={}),\n  AIMessage(content='whats up', additional_kwargs={})]}\nUsing in a chain#\nFinally, let\u2019s take a look at using this in a chain (setting verbose=True so we can see the prompt).\nfrom langchain.llms import OpenAI\nfrom langchain.chains import ConversationChain\nllm = OpenAI(temperature=0)\nconversation = ConversationChain(\n    llm=llm, \n    verbose=True, \n    memory=ConversationBufferMemory()\n)\nconversation.predict(input=\"Hi there!\")\n> Entering new ConversationChain chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\nCurrent conversation:\nHuman: Hi there!\nAI:\n> Finished chain.\n\n---\n\n.ipynb\n.pdf\nGetting Started\n Contents \nChatMessageHistory\nConversationBufferMemory\nUsing in a chain\nSaving Message History\nGetting Started#\nThis notebook walks through how LangChain thinks about memory.\nMemory involves keeping a concept of state around throughout a user\u2019s interactions with an language model. A user\u2019s interactions with a language model are captured in the concept of ChatMessages, so this boils down to ingesting, capturing, transforming and extracting knowledge from a sequence of chat messages. There are many different ways to do this, each of which exists as its own memory type.\nIn general, for each type of memory there are two ways to understanding using memory. These are the standalone functions which extract information from a sequence of messages, and then there is the way you can use this type of memory in a chain.\nMemory can return multiple pieces of information (for example, the most recent N messages and a summary of all previous messages). The returned information can either be a string or a list of messages.\nIn this notebook, we will walk through the simplest form of memory: \u201cbuffer\u201d memory, which just involves keeping a buffer of all prior messages. We will show how to use the modular utility functions here, then show how it can be used in a chain (both returning a string as well as a list of messages).\nChatMessageHistory#\nOne of the core utility classes underpinning most (if not all) memory modules is the ChatMessageHistory class. This is a super lightweight wrapper which exposes convienence methods for saving Human messages, AI messages, and then fetching them all.\nYou may want to use this class directly if you are managing memory outside of a chain.\nfrom langchain.memory import ChatMessageHistory\nhistory = ChatMessageHistory()\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\nhistory.messages\n[HumanMessage(content='hi!', additional_kwargs={}),\n\n---\n\n.ipynb\n.pdf\nConversationTokenBufferMemory\n Contents \nUsing in a chain\nConversationTokenBufferMemory#\nConversationTokenBufferMemory keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions.\nLet\u2019s first walk through how to use the utilities\nfrom langchain.memory import ConversationTokenBufferMemory\nfrom langchain.llms import OpenAI\nllm = OpenAI()\nmemory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10)\nmemory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\nmemory.save_context({\"input\": \"not much you\"}, {\"ouput\": \"not much\"})\nmemory.load_memory_variables({})\n{'history': 'Human: not much you\\nAI: not much'}\nWe can also get the history as a list of messages (this is useful if you are using this with a chat model).\nmemory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10, return_messages=True)\nmemory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\nmemory.save_context({\"input\": \"not much you\"}, {\"ouput\": \"not much\"})\nUsing in a chain#\nLet\u2019s walk through an example, again setting verbose=True so we can see the prompt.\nfrom langchain.chains import ConversationChain\nconversation_with_summary = ConversationChain(\n    llm=llm, \n    # We set a very low max_token_limit for the purposes of testing.\n    memory=ConversationTokenBufferMemory(llm=OpenAI(), max_token_limit=60),\n    verbose=True\n)\nconversation_with_summary.predict(input=\"Hi, what's up?\")\n> Entering new ConversationChain chain...\nPrompt after formatting:\n\nQuestion: Tell me about ConversationBufferMemory in langchain, and walk me through how to use it.\nAnswer:", "Tell me about ConversationBufferMemory in langchain, and walk me through how to use it.", "ConversationBufferMemory is a memory module in LangChain that keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions. It can be used in a chain by setting the memory parameter to ConversationBufferMemory when creating a ConversationChain object. The memory can return multiple pieces of information (for example, the most recent N messages and a summary of all previous messages) as either a string or a list of messages."]]}